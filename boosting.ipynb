{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees: Ensemble Methods - Boosting\n",
    "\n",
    "Boosting is another ensemble technique to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analyzing data for errors. In other words, we fit consecutive trees (random sample) at every step,and the goal is to solve for net error from the prior tree.\n",
    "\n",
    "When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. By combining the whole set at the end converts weak learners into a better performing model.\n",
    "\n",
    "An ensemble of trees are built one by one and individual trees are summed sequentially. The Next tree tries to recover the loss (difference between actual and predicted values) from the previous tree.\n",
    "\n",
    " - boosting = low variance, high bias base learners\n",
    " \n",
    " ![Boosting Example](./images/boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost = Adaptive Boosting\n",
    "AdaBoost learns from the mistakes by increasing the weight of misclassified data points.\n",
    "\n",
    "It is called Adaptive Boosting as the weights are re-assigned to each instance, with higher weights to incorrectly classified instances.\n",
    "\n",
    "*Adaboost usually has just a node and two leaves.(A tree with one node and two leaves is called a stump)*\n",
    "\n",
    "Steps:\n",
    "<li> 0: Initialize the weights of data points. (e.g. data has 1000 points, each initial point would have 1/1000 = 0.001) </li>\n",
    "<li> 1: Train a decision Tree (whole dataset) </li>\n",
    "<li> 2: Calculate the weighted error rate (e) of the decision tree. </li>\n",
    "<li> 3: Calculate this decision tree’s weight in the ensemble. The weight of this tree = learning rate * log( (1 — e) / e) </li> \n",
    "<br> ** The higher the weighted error of the tree, the less decision power the tree will be given during the later voting. </br>\n",
    "<br> ** The lower the weighted error of the tree, the higher decision power the tree will be given during the later voting. </br>\n",
    "\n",
    "<li> 4: Update weights of wrongly classified points. </li> \n",
    "<br> the weight of each data point stays same if the model got this data points correct.</br>\n",
    "<br> the <strong><em>new weight of this data point = old weight*exp(weight of the tree)</em></strong>, if the model got this data point wrong </br> \n",
    "\n",
    "![sample weight calculation](./images/sample_weight_calc.png)\n",
    "\n",
    "** The amount of say (alpha) will be negative when the sample is correctly classified.\n",
    "\n",
    "** The amount of say (alpha) will be positive when the sample is miss-classified.\n",
    "\n",
    "--- We normalize weights to bring them all to the sum of one afterwards.\n",
    "\n",
    "<li> 5: Repeat step 1 (dataset with new weights) </li>\n",
    "<li> 6: Make final prediction </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading:https://www.mygreatlearning.com/blog/adaboost-algorithm/\n",
    "<br> https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/#:~:text=AdaBoost%20also%20called%20Adaptive%20Boosting,are%20also%20called%20Decision%20Stumps </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting = Gradient Descent + Boosting.\n",
    "Gradient Descent is a first-order iterative optimization algorithm for finding a local minimum of a differential function. If x(n+1) = x(n) - learning_rate*dF/dx(n) for a small learning_rate, then F(x(n)) => F(x(n+1)). (the idea is to move against the gradient).\n",
    "\n",
    "Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of changing the weights for every incorrect classified observation at every iteration like AdaBoost, Gradient Boosting method tries to fit the new predictor to the residual errors made by the previous predictor.\n",
    "\n",
    "Say we have mean squared error (MSE) as loss defined as:\n",
    "![Mean squared error](./images/xgb_1.png)\n",
    "\n",
    "We want our predictions, such that our loss function (MSE) is minimum. By using gradient descent and updating our predictions based on a learning rate, we can find the values where MSE is minimum.\n",
    "![gradient boosting](./images/xgb_2.png)\n",
    "\n",
    "So, we are basically updating the predictions such that the sum of our residuals is close to 0 (or minimum) and predicted values are sufficiently close to actual values.\n",
    "\n",
    "<strong>Note:</strong>\n",
    "\n",
    "<li> Gradient Boosting is prone to Over-fitting.</li>\n",
    "<li> Requires careful tuning of different hyper-parameters.</li>\n",
    "\n",
    "Example: https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import catboost as cb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:55:57] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "--- 0.05968928337097168 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.583590106471756"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dataset\n",
    "\n",
    "X,y = load_boston(return_X_y=True)\n",
    "\n",
    "#train,test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "#xgboost\n",
    "xgbr = xgb.XGBRegressor(max_depth=5,learning_rate=0.1,n_estimators=100,n_jobs=-1)\n",
    "start_time = time.time()  #track the model development time\n",
    "\n",
    "xgbr.fit(X_train,y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "y_predict = xgbr.predict(X_test)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time)) \n",
    "\n",
    "mean_squared_error(y_test,y_predict) #error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.09108877182006836 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.069578290965865"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets try lightgbm\n",
    "#it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise.\n",
    "\n",
    "lgbr = lgb.LGBMRegressor(learning_rate=0.1,n_estimators=100,max_depth=5,num_leaves=50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lgbr.fit(X_train,y_train,verbose=0)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "y_predict = lgbr.predict(X_test)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "\n",
    "mean_squared_error(y_test,y_predict)    #error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.19351601600646973 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.344821856482579"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#catboost helps you savetime by preprocessing of categorical columns for you.\n",
    "#weighted sampling version of Stochastic Gradient Boosting.\n",
    "\n",
    "#lets try catboost\n",
    "cbr = cb.CatBoostRegressor(learning_rate=0.1,n_estimators=100,max_depth=5)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cbr.fit(X_train,y_train,verbose=0)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "y_predict = cbr.predict(X_test)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (end_time - start_time))\n",
    "\n",
    "mean_squared_error(y_test,y_predict)    #error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Load the promotion dataset from the data folder, train a model on the dataset and compare results using both random forests and gradient boosting.\n",
    "\n",
    "<strong>Note: Also make sure to do some data cleaning, upsampling/downsampling, parameter tuning.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_estimators`\n",
    "- increasing num trees will increase model complexity\n",
    "\n",
    "`max_features`\n",
    "- how many features to split on\n",
    "- rule of thumb = sqrt(num_features)\n",
    "- depends on ratio of noisy to important var in dataset\n",
    "- small num features = reduce variance increase bias\n",
    "- lots of noisy = small m will decrease probability of choosing an important variable at a split\n",
    "\n",
    "`min samples per leaf` \n",
    "- increase a bit (default is 1) to get smaller trees w less overfitting\n",
    "\n",
    "`max_depth`\n",
    "- controls variance\n",
    "\n",
    "`subsample`\n",
    "- The fraction of observations to be selected for each tree. Selection is done by random sampling.\n",
    "- Values slightly less than 1 make the model robust by reducing the variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting point hyperparameters\n",
    "\n",
    "*** Heard from a Kaggle Grandmaster\n",
    "\n",
    "Learning rate = 0.05, 1000 rounds, max depth = 3-5, subsample = 0.8-1.0, colsample_bytree = 0.3 - 0.8, lambda = 0 to 5\n",
    "\n",
    "Add capacity to combat bias - add rounds\n",
    "\n",
    "Reduce capacity to combat variance - depth / regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeNo</th>\n",
       "      <th>Division</th>\n",
       "      <th>Qualification</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Channel_of_Recruitment</th>\n",
       "      <th>Trainings_Attended</th>\n",
       "      <th>Year_of_birth</th>\n",
       "      <th>Last_performance_score</th>\n",
       "      <th>Year_of_recruitment</th>\n",
       "      <th>Targets_met</th>\n",
       "      <th>Previous_Award</th>\n",
       "      <th>Training_score_average</th>\n",
       "      <th>State_Of_Origin</th>\n",
       "      <th>Foreign_schooled</th>\n",
       "      <th>Marital_Status</th>\n",
       "      <th>Past_Disciplinary_Action</th>\n",
       "      <th>Previous_IntraDepartmental_Movement</th>\n",
       "      <th>No_of_previous_employers</th>\n",
       "      <th>Promoted_or_Not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YAK/S/00001</td>\n",
       "      <td>Commercial Sales and Marketing</td>\n",
       "      <td>MSc, MBA and PhD</td>\n",
       "      <td>Female</td>\n",
       "      <td>Direct Internal process</td>\n",
       "      <td>2</td>\n",
       "      <td>1986</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>ANAMBRA</td>\n",
       "      <td>No</td>\n",
       "      <td>Married</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YAK/S/00002</td>\n",
       "      <td>Customer Support and Field Operations</td>\n",
       "      <td>First Degree or HND</td>\n",
       "      <td>Male</td>\n",
       "      <td>Agency and others</td>\n",
       "      <td>2</td>\n",
       "      <td>1991</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>ANAMBRA</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Married</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YAK/S/00003</td>\n",
       "      <td>Commercial Sales and Marketing</td>\n",
       "      <td>First Degree or HND</td>\n",
       "      <td>Male</td>\n",
       "      <td>Direct Internal process</td>\n",
       "      <td>2</td>\n",
       "      <td>1987</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>KATSINA</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Married</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YAK/S/00004</td>\n",
       "      <td>Commercial Sales and Marketing</td>\n",
       "      <td>First Degree or HND</td>\n",
       "      <td>Male</td>\n",
       "      <td>Agency and others</td>\n",
       "      <td>3</td>\n",
       "      <td>1982</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>NIGER</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Single</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YAK/S/00006</td>\n",
       "      <td>Information and Strategy</td>\n",
       "      <td>First Degree or HND</td>\n",
       "      <td>Male</td>\n",
       "      <td>Direct Internal process</td>\n",
       "      <td>3</td>\n",
       "      <td>1990</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>AKWA IBOM</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Married</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    EmployeeNo                               Division        Qualification  \\\n",
       "0  YAK/S/00001         Commercial Sales and Marketing     MSc, MBA and PhD   \n",
       "1  YAK/S/00002  Customer Support and Field Operations  First Degree or HND   \n",
       "2  YAK/S/00003         Commercial Sales and Marketing  First Degree or HND   \n",
       "3  YAK/S/00004         Commercial Sales and Marketing  First Degree or HND   \n",
       "4  YAK/S/00006               Information and Strategy  First Degree or HND   \n",
       "\n",
       "   Gender   Channel_of_Recruitment  Trainings_Attended  Year_of_birth  \\\n",
       "0  Female  Direct Internal process                   2           1986   \n",
       "1    Male        Agency and others                   2           1991   \n",
       "2    Male  Direct Internal process                   2           1987   \n",
       "3    Male        Agency and others                   3           1982   \n",
       "4    Male  Direct Internal process                   3           1990   \n",
       "\n",
       "   Last_performance_score  Year_of_recruitment  Targets_met  Previous_Award  \\\n",
       "0                    12.5                 2011            1               0   \n",
       "1                    12.5                 2015            0               0   \n",
       "2                     7.5                 2012            0               0   \n",
       "3                     2.5                 2009            0               0   \n",
       "4                     7.5                 2012            0               0   \n",
       "\n",
       "   Training_score_average State_Of_Origin Foreign_schooled Marital_Status  \\\n",
       "0                      41         ANAMBRA               No        Married   \n",
       "1                      52         ANAMBRA              Yes        Married   \n",
       "2                      42         KATSINA              Yes        Married   \n",
       "3                      42           NIGER              Yes         Single   \n",
       "4                      77       AKWA IBOM              Yes        Married   \n",
       "\n",
       "  Past_Disciplinary_Action Previous_IntraDepartmental_Movement  \\\n",
       "0                       No                                  No   \n",
       "1                       No                                  No   \n",
       "2                       No                                  No   \n",
       "3                       No                                  No   \n",
       "4                       No                                  No   \n",
       "\n",
       "  No_of_previous_employers  Promoted_or_Not  \n",
       "0                        0                0  \n",
       "1                        0                0  \n",
       "2                        0                0  \n",
       "3                        1                0  \n",
       "4                        1                0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/promotion/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
