{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Trees: Ensemble Methods - Bagging\n",
    "\n",
    "Bagging: Training a bunch of individual models in a parallel way. Each model is trained by a random subset of the data. (Summary!)\n",
    "\n",
    "BAGGing stands for Bootstrapping(sampling with replacement) and AGGregating (Averaging predictions).\n",
    "\n",
    "### <strong> Random Forest </strong>\n",
    "\n",
    "With Random Forest in addition to taking the random subset of data, it also takes the random selection of features rather than using all features to grow trees. When you have many random trees. It’s called Random Forest.\n",
    "\n",
    "With Random Forest, our goal is to reduce the variance of a decision Tree. We end up with an ensemble of different models. Average of all the predictions from different trees are used which is more robust than a single decision tree.\n",
    "\n",
    "- forests = high variance, low bias base learners\n",
    "- Bagging to decrease the model’s variance\n",
    "\n",
    "<img src=\"./images/boostrap_aggregating.png\" width=\"500\" height=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> Extremely Randomized Trees </strong>\n",
    "\n",
    "Extremely Randomized Trees, abbreviated as ExtraTrees in Sklearn, adds one more step of randomization to the random forest algorithm. \n",
    "\n",
    "Random forests will \n",
    "\n",
    "1. compute the optimal split to make for each feature within the randomly selected subset, and it will then choose the best feature to split on. \n",
    "2. builds multiple trees with bootstrap = True (by default), which means it samples replacement.\n",
    "\n",
    "ExtraTrees on the other hand(compared to Random Forests) will instead choose a random split to make for each feature within that random subset, and it will subsequently choose the best feature to split on by comparing those randomly chosen splits. (nodes are split on random splits, not best splits.)\n",
    "\n",
    "Like random forest, the Extra Trees algorithm will randomly sample the features at each split point of a decision tree. Unlike random forest, which uses a greedy algorithm to select an optimal split point, the Extra Trees algorithm selects a split point at random.\n",
    "\n",
    "In terms of computational cost, and therefore execution time, the Extra Trees algorithm is faster. This algorithm saves time because the whole procedure is the same, but it randomly chooses the split point and does not calculate the optimal one.\n",
    "\n",
    "Extremely randomized trees are much more computationally efficient than random forests, and their performance is almost always comparable. In some cases, they may even perform better!\n",
    "\n",
    "![Bagging](./images/rf_extra.png)\n",
    "\n",
    "Link to Paper: https://link.springer.com/content/pdf/10.1007/s10994-006-6226-1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "\n",
    "X,y = load_iris(return_X_y=True)\n",
    "\n",
    "#train,test split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42)\n",
    "\n",
    "#random forest with gini\n",
    "rf = RandomForestClassifier(criterion='gini',n_estimators=150,max_depth=4,n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train,y_train)  #fit on the data\n",
    "\n",
    "rf_predict = rf.predict(X_test)\n",
    "\n",
    "f1_score(y_test, rf_predict, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest with gini\n",
    "rf = RandomForestClassifier(criterion='entropy',n_estimators=200,max_depth=4,n_jobs=-1)\n",
    "\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "rf_predict = rf.predict(X_test)\n",
    "\n",
    "f1_score(y_test, rf_predict, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Using the data in scout_data, build a model to predict a product tier(Classification) and a model to predict the number of detail views.(Regression)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
